{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models: A Comprehensive Overview\n",
    "\n",
    "In this notebook, we will explore various machine learning models including linear regression, logistic regression, decision trees, Naive Bayes, and K-means clustering. We will also visualize assumptions and model performance metrics. \n",
    "\n",
    "The notebook is designed to be flexible for future use, allowing for easy adaptation to different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports and Data Loading\n",
    "\n",
    "In this section, we import all the necessary packages and libraries for data manipulation, machine learning models, and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for machine learning models, data manipulation, and visualizations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# For advanced machine learning algorithms\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "We will load the dataset into a pandas DataFrame and inspect the first few rows to understand the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (replace 'your_dataset.csv' with actual dataset path)\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# This lets us see all of the columns, preventing Juptyer from redacting them.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n",
    "\n",
    "# Check for missing values or data types\n",
    "df.info()\n",
    "\n",
    "\n",
    "# Transform data if needed\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before we dive into modeling, it's important to understand the distribution and relationships in the dataset. We'll start by plotting the distributions of numerical features and checking for correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of all numerical columns\n",
    "df.hist(bins=20, figsize=(14, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check correlations between features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Advanced EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataAnalyzer:\n",
    "    \"\"\"\n",
    "    A class to perform Exploratory Data Analysis (EDA) and data transformations on a dataset,\n",
    "    including handling and detecting missing data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize with the DataFrame to analyze.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def standardize_missing_values(self):\n",
    "        \"\"\"\n",
    "        Standardize common missing value representations (e.g., \"NA\", \"Null\", \".\", \" \") to NaN.\n",
    "        \"\"\"\n",
    "        missing_values = [\"NA\", \"Null\", \".\", \" \"]  # Define common missing value representations\n",
    "        self.df.replace(missing_values, np.nan, inplace=True)  # Replace them with NaN\n",
    "        print(\"Standardized missing values to NaN.\")\n",
    "\n",
    "    def check_missing_data(self):\n",
    "        \"\"\"\n",
    "        Check for missing values and empty cells in the dataset.\n",
    "        \"\"\"\n",
    "        print(\"=== Null Value Check ===\")\n",
    "        null_values = self.df.isnull().sum()\n",
    "        print(null_values[null_values > 0])\n",
    "\n",
    "        print(\"\\n=== Empty Cell Check ===\")\n",
    "        empty_cells = (self.df == '').sum()\n",
    "        print(empty_cells[empty_cells > 0])\n",
    "\n",
    "    def visualize_missing_data(self):\n",
    "        \"\"\"\n",
    "        Visualize missing data as a heatmap.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(self.df.isnull(), cbar=False, cmap='viridis')\n",
    "        plt.title(\"Missing Data Heatmap\")\n",
    "        plt.show()\n",
    "\n",
    "    def handle_missing_data(self, strategy=\"drop\", fill_value=None, method=None):\n",
    "        \"\"\"\n",
    "        Handle missing data by dropping or filling based on a specified strategy.\n",
    "        \n",
    "        :param strategy: \"drop\", \"fill\", or \"fill_method\". Default is \"drop\".\n",
    "        :param fill_value: Value to use for filling missing data when strategy=\"fill\" (e.g., mean, median, custom value).\n",
    "        :param method: For forward or backward filling, use \"ffill\" (forward fill) or \"bfill\" (backward fill).\n",
    "        :return: Modified DataFrame with handled missing data.\n",
    "        \"\"\"\n",
    "        if strategy == \"drop\":\n",
    "            # Drop rows with any missing values\n",
    "            print(\"Dropping rows with missing data...\")\n",
    "            self.df = self.df.dropna()\n",
    "\n",
    "        elif strategy == \"fill\":\n",
    "            # Fill missing data with specified fill_value\n",
    "            if fill_value == 'mean':\n",
    "                print(\"Filling missing values with column means...\")\n",
    "                self.df = self.df.fillna(self.df.mean())\n",
    "            elif fill_value == 'median':\n",
    "                print(\"Filling missing values with column medians...\")\n",
    "                self.df = self.df.fillna(self.df.median())\n",
    "            elif fill_value == 'mode':\n",
    "                print(\"Filling missing values with column modes...\")\n",
    "                self.df = self.df.fillna(self.df.mode().iloc[0])\n",
    "            else:\n",
    "                print(f\"Filling missing values with {fill_value}...\")\n",
    "                self.df = self.df.fillna(fill_value)\n",
    "\n",
    "        elif strategy == \"fill_method\":\n",
    "            # Forward or backward fill based on method\n",
    "            if method == \"ffill\":\n",
    "                print(\"Forward filling missing values...\")\n",
    "                self.df = self.df.fillna(method='ffill')\n",
    "            elif method == \"bfill\":\n",
    "                print(\"Backward filling missing values...\")\n",
    "                self.df = self.df.fillna(method='bfill')\n",
    "            else:\n",
    "                print(\"Invalid method. Use 'ffill' for forward fill or 'bfill' for backward fill.\")\n",
    "\n",
    "        print(\"Missing data handled.\")\n",
    "        return self.df\n",
    "\n",
    "    def data_summary(self):\n",
    "        \"\"\"\n",
    "        Print a summary of the dataset, including general info and statistical summary.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Data Info ===\")\n",
    "        print(self.df.info())\n",
    "\n",
    "        print(\"\\n=== Summary Statistics ===\")\n",
    "        print(self.df.describe())\n",
    "\n",
    "    def transpose_data(self):\n",
    "        \"\"\"\n",
    "        Transpose the dataset for easier viewing of wide datasets.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Transposed Data ===\")\n",
    "        print(self.df.transpose().head())\n",
    "\n",
    "    def visualize_feature_distributions(self):\n",
    "        \"\"\"\n",
    "        Visualize distributions of numerical features using histograms.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Feature Distributions ===\")\n",
    "        self.df.hist(bins=20, figsize=(14, 10))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_feature_distributions(self):\n",
    "        \"\"\"\n",
    "        Visualize distributions of numerical features using histograms.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Feature Distributions ===\")\n",
    "        self.df.hist(bins=20, figsize=(14, 10))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def detect_outliers_with_boxplot(self):\n",
    "        \"\"\"\n",
    "        Detect outliers visually using a boxplot for each numerical feature.\n",
    "        Outliers are points outside the whiskers of the boxplot.\n",
    "        \"\"\"\n",
    "        numerical_columns = self.df.select_dtypes(include=[np.number]).columns\n",
    "        for column in numerical_columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.boxplot(x=self.df[column])\n",
    "            plt.title(f'Boxplot of {column}')\n",
    "            plt.show()\n",
    "\n",
    "    def detect_outliers_with_zscore(self):\n",
    "        \"\"\"\n",
    "        Detect outliers using Z-scores for numerical columns.\n",
    "        Outliers are points with a Z-score greater than 3.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Outlier Detection (Z-score > 3) ===\")\n",
    "        z_scores = np.abs(stats.zscore(self.df.select_dtypes(include=[np.number])))\n",
    "        outliers = (z_scores > 3).sum(axis=0)\n",
    "        print(outliers[outliers > 0])\n",
    "\n",
    "    def apply_log_transformation(self):\n",
    "        \"\"\"\n",
    "        Apply a log transformation to numerical columns.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Log Transformation ===\")\n",
    "        df_log = self.df.select_dtypes(include=[np.number]).apply(lambda x: np.log(x + 1))\n",
    "        df_log.hist(bins=20, figsize=(14, 10))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def apply_standard_scaling(self):\n",
    "        \"\"\"\n",
    "        Apply standard scaling (z-score normalization) to numerical columns.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Standard Scaling ===\")\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = pd.DataFrame(scaler.fit_transform(self.df.select_dtypes(include=[np.number])), columns=self.df.select_dtypes(include=[np.number]).columns)\n",
    "        df_scaled.hist(bins=20, figsize=(14, 10))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def apply_minmax_scaling(self):\n",
    "        \"\"\"\n",
    "        Apply Min-Max scaling (normalization) to numerical columns.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== MinMax Scaling ===\")\n",
    "        minmax_scaler = MinMaxScaler()\n",
    "        df_minmax = pd.DataFrame(minmax_scaler.fit_transform(self.df.select_dtypes(include=[np.number])), columns=self.df.select_dtypes(include=[np.number]).columns)\n",
    "        df_minmax.hist(bins=20, figsize=(14, 10))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data into Training, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size=0.3, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the dataset into train, validation, and test sets.\n",
    "    \n",
    "    :param X: Features\n",
    "    :param y: Target\n",
    "    :param test_size: Size of the test set\n",
    "    :param val_size: Size of the validation set (relative to the train set)\n",
    "    :param random_state: Seed for reproducibility\n",
    "    :return: Split dataset into X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    # Initial split into train and test sets\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Further split the training set into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=val_size, random_state=random_state)\n",
    "\n",
    "    print(f\"Training set shape: {X_train.shape}, Validation set shape: {X_val.shape}, Test set shape: {X_test.shape}\")\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Linear Regression with Assumptions Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Fit a linear regression model and check assumptions.\n",
    "    \n",
    "    :return: Fitted linear model, validation predictions\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Validate on the validation set\n",
    "    val_preds = model.predict(X_val)\n",
    "    \n",
    "    # Plot residuals\n",
    "    plt.scatter(val_preds, y_val - val_preds)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals vs Predicted')\n",
    "    plt.show()\n",
    "    \n",
    "    # QQ plot for residuals\n",
    "    sm.qqplot(y_val - val_preds, line='s')\n",
    "    plt.title('QQ Plot for Residuals')\n",
    "    plt.show()\n",
    "\n",
    "    return model, val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df.drop(columns='target_column')  # Features\n",
    "y = df['target_column']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and fit the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "# Plot Residuals to Check Linearity Assumption\n",
    "plt.scatter(y_pred, y_test - y_pred)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "# Run a quick iteration to inspect the coefficients\n",
    "for i, coef in enumerate(linear_model.coef_):\n",
    "    print(f\"Coefficient for feature {X.columns[i]}: {coef}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with ROC Curve\n",
    "\n",
    "Logistic regression is used for binary classification tasks. In this example, weâ€™ll fit a logistic regression model, plot an ROC curve, and compute the area under the curve (AUC) to evaluate model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "def logistic_regression(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Fit a logistic regression model and plot ROC curve.\n",
    "    \n",
    "    :return: Fitted logistic model, validation predictions\n",
    "    \"\"\"\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities for the validation set\n",
    "    val_probs = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_val, val_probs)\n",
    "    plt.plot(fpr, tpr, label='ROC Curve')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Random chance line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show()\n",
    "\n",
    "    auc = roc_auc_score(y_val, val_probs)\n",
    "    print(f\"AUC: {auc:.3f}\")\n",
    "\n",
    "    return model, val_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit the logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "logistic_preds = logistic_model.predict(X_test)\n",
    "logistic_probs = logistic_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, logistic_probs)\n",
    "plt.plot(fpr, tpr, label=\"Logistic Regression\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random chance\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# Print AUC score for better evaluation\n",
    "auc_score = roc_auc_score(y_test, logistic_probs)\n",
    "print(f\"Logistic Regression AUC: {auc_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_lasso_regression(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Fit and tune Ridge and Lasso models using validation set.\n",
    "    \n",
    "    :return: Best Ridge and Lasso models\n",
    "    \"\"\"\n",
    "    # Ridge Regression\n",
    "    ridge = Ridge()\n",
    "    lasso = Lasso()\n",
    "    \n",
    "    param_grid = {'alpha': [0.01, 0.1, 1, 10]}  # Regularization strength\n",
    "    \n",
    "    ridge_cv = GridSearchCV(ridge, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    lasso_cv = GridSearchCV(lasso, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    ridge_cv.fit(X_train, y_train)\n",
    "    lasso_cv.fit(X_train, y_train)\n",
    "    \n",
    "    # Validate on the validation set\n",
    "    ridge_val_preds = ridge_cv.predict(X_val)\n",
    "    lasso_val_preds = lasso_cv.predict(X_val)\n",
    "\n",
    "    print(f\"Best Ridge Alpha: {ridge_cv.best_params_}\")\n",
    "    print(f\"Best Lasso Alpha: {lasso_cv.best_params_}\")\n",
    "\n",
    "    return ridge_cv, lasso_cv, ridge_val_preds, lasso_val_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params\n",
    "n_estimators: The number of trees or boosting rounds used.\n",
    "learning_rate: Controls the contribution of each tree in the boosting process.\n",
    "max_depth: The maximum depth of individual trees (or decision rules).\n",
    "min_samples_split and min_samples_leaf: Control how the tree grows and when it stops growing.\n",
    "subsample and colsample_bytree: Control the fraction of samples and features used for each tree, which can prevent overfitting.\n",
    "max_features: Controls the number of features considered for each split.\n",
    "min_child_weight and min_child_samples: Control the minimum number of instances or weight required to form a node in tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_regression(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Fit a Random Forest Regressor and tune hyperparameters using GridSearchCV.\n",
    "    \n",
    "    :return: Best Random Forest model, validation predictions\n",
    "    \"\"\"\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "    # Expanded parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300, 500],  # Number of trees in the forest\n",
    "        'max_depth': [None, 5, 10, 20],  # Maximum depth of the tree\n",
    "        'min_samples_split': [2, 10, 20],  # Minimum samples required to split a node\n",
    "        'min_samples_leaf': [1, 2, 4],  # Minimum samples required to be at a leaf node\n",
    "        'max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider for the best split\n",
    "    }\n",
    "    \n",
    "    rf_cv = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    rf_cv.fit(X_train, y_train)\n",
    "    \n",
    "    # Validate on validation set\n",
    "    val_preds = rf_cv.predict(X_val)\n",
    "    \n",
    "    print(f\"Best Random Forest Parameters: {rf_cv.best_params_}\")\n",
    "    return rf_cv, val_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_regression(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Fit a Gradient Boosting Regressor and tune hyperparameters.\n",
    "    \n",
    "    :return: Best Gradient Boosting model, validation predictions\n",
    "    \"\"\"\n",
    "    gb = GradientBoostingRegressor(random_state=42)\n",
    "    \n",
    "    # Expanded parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],  # Number of boosting stages\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate shrinks the contribution of each tree\n",
    "        'max_depth': [3, 5, 7],  # Maximum depth of the individual regression estimators\n",
    "        'min_samples_split': [2, 10, 20],  # Minimum samples required to split a node\n",
    "        'min_samples_leaf': [1, 2, 4],  # Minimum samples required at a leaf node\n",
    "        'subsample': [0.7, 0.8, 1.0],  # Fraction of samples used for fitting each base learner\n",
    "        'max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider for the best split\n",
    "    }\n",
    "    \n",
    "    gb_cv = GridSearchCV(gb, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    gb_cv.fit(X_train, y_train)\n",
    "    \n",
    "    # Validate on validation set\n",
    "    val_preds = gb_cv.predict(X_val)\n",
    "    \n",
    "    print(f\"Best Gradient Boosting Parameters: {gb_cv.best_params_}\")\n",
    "    return gb_cv, val_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_regression(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Fit an XGBoost Regressor and tune hyperparameters.\n",
    "    \n",
    "    :return: Best XGBoost model, validation predictions\n",
    "    \"\"\"\n",
    "    xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    \n",
    "    # Expanded parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],  # Number of boosting rounds\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate\n",
    "        'max_depth': [3, 5, 7],  # Maximum depth of a tree\n",
    "        'min_child_weight': [1, 3, 5],  # Minimum sum of instance weight needed in a child\n",
    "        'subsample': [0.7, 0.8, 1.0],  # Fraction of samples used for fitting each tree\n",
    "        'colsample_bytree': [0.7, 0.8, 1.0]  # Fraction of features used at each tree split\n",
    "    }\n",
    "    \n",
    "    xgb_cv = GridSearchCV(xgb, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    xgb_cv.fit(X_train, y_train)\n",
    "    \n",
    "    # Validate on validation set\n",
    "    val_preds = xgb_cv.predict(X_val)\n",
    "    \n",
    "    print(f\"Best XGBoost Parameters: {xgb_cv.best_params_}\")\n",
    "    return xgb_cv, val_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_regression(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Fit a LightGBM Regressor and tune hyperparameters.\n",
    "    \n",
    "    :return: Best LightGBM model, validation predictions\n",
    "    \"\"\"\n",
    "    lgbm = LGBMRegressor(random_state=42)\n",
    "    \n",
    "    # Expanded parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],  # Number of boosting rounds\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate\n",
    "        'max_depth': [-1, 5, 10],  # Maximum depth of the trees (-1 for no limit)\n",
    "        'num_leaves': [31, 50, 100],  # Maximum number of leaves in one tree\n",
    "        'min_child_samples': [20, 50, 100],  # Minimum number of data points in a child\n",
    "        'subsample': [0.7, 0.8, 1.0],  # Fraction of samples used for fitting each base learner\n",
    "        'colsample_bytree': [0.7, 0.8, 1.0]  # Fraction of features used at each tree split\n",
    "    }\n",
    "    \n",
    "    lgbm_cv = GridSearchCV(lgbm, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    lgbm_cv.fit(X_train, y_train)\n",
    "    \n",
    "    # Validate on validation set\n",
    "    val_preds = lgbm_cv.predict(X_val)\n",
    "    \n",
    "    print(f\"Best LightGBM Parameters: {lgbm_cv.best_params_}\")\n",
    "    return lgbm_cv, val_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "Decision trees are highly interpretable models that split the dataset based on feature values. In this section, we use `GridSearchCV` to tune the tree's hyperparameters and evaluate feature importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Decision Tree Classifier and hyperparameters\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 10],\n",
    "    'min_samples_leaf': [1, 5]\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning with GridSearchCV\n",
    "grid_search = GridSearchCV(tree, param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model from Grid Search\n",
    "best_tree = grid_search.best_estimator_\n",
    "\n",
    "# Feature importance\n",
    "importances = pd.Series(best_tree.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "importances.plot(kind='bar', title='Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "K-Means clustering is an unsupervised learning algorithm used to group similar data points together. We will use the inertia and silhouette score to evaluate the quality of the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Calculate Inertia and Silhouette Score\n",
    "inertia = kmeans.inertia_\n",
    "silhouette_avg = silhouette_score(X_scaled, kmeans.labels_)\n",
    "\n",
    "print(f\"Inertia: {inertia}\")\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Plot clusters (if X is 2-dimensional)\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "plt.title('K-Means Clustering Results')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "def hierarchical_clustering(X):\n",
    "    # Use Ward's method for hierarchical clustering\n",
    "    Z = linkage(X, method='ward')\n",
    "    \n",
    "    # Plot dendrogram\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    dendrogram(Z)\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "    plt.xlabel('Data Points')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN (Density-Based Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def dbscan_clustering(X):\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    labels = dbscan.fit_predict(X)\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "    plt.title('DBSCAN Clustering Results')\n",
    "    plt.show()\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def gaussian_mixture_model(X, n_components=3):\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "    gmm.fit(X)\n",
    "    \n",
    "    labels = gmm.predict(X)\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "    plt.title('Gaussian Mixture Model Clustering')\n",
    "    plt.show()\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes is a simple yet effective algorithm for classification. We'll fit a Gaussian Naive Bayes model and evaluate the model's performance using a confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit the Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "nb_preds = nb_model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, nb_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# Get accuracy, precision, recall, and F1 score for the model\n",
    "nb_test_scores = get_test_scores('Naive Bayes', nb_preds, y_test)\n",
    "print(nb_test_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "In this notebook, we explored several machine learning models including linear regression, logistic regression, decision trees, K-means clustering, and Naive Bayes. We also discussed how to check for key statistical assumptions and used different evaluation metrics like AUC and silhouette score to assess the models. \n",
    "\n",
    "This structure can be easily adapted for different datasets by simply updating the data loading step and the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the decision splits for an initial small tree iteration\n",
    "print(\"Initial Tree Iteration Splits:\")\n",
    "for i, feature in enumerate(best_tree.tree_.feature):\n",
    "    if feature != -2:  # -2 means it's a leaf node\n",
    "        print(f\"Node {i}: Split on feature {X.columns[feature]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
